# -*- coding: utf-8 -*-
"""IC do Renato.ipynb

Automatically generated by Colaboratory.

#Ambiente
"""

import tensorflow as tf
import keras
from keras.models import Sequential, Model
from keras.layers import Input, Dense, CuDNNGRU, Embedding, Dropout,Flatten, Activation, RepeatVector, Activation, multiply
from keras.optimizers import Adam, RMSprop
from keras.regularizers import l2
from keras.callbacks import TensorBoard
import matplotlib.pyplot as plt
from collections import deque
import numpy as np
import copy
import random
import json

class DQN():
  def __init__(self):
    self.memory = deque(maxlen=2000)
    self.gamma = 0.0001
    self.e = 0.0
    self.e_decay = 0.99
    self.e_min = 0.1
    self.LR = 1e-4
    self.n_estados = 770 + 256 + 256
    self.n_servers = 256
    self.model = self.build_model()
    #self.model = self.build_model(self.n_estados, self.n_servers)

  def build_model(self):
    model = Sequential()
    model.add(Dense(128, input_dim = self.n_estados, activation="relu"))
    for _ in range(4):
      model.add(Dense(256, activation="relu"))
    
    model.add(Dense(self.n_servers, activation='softmax'))
    model.compile(optimizer=Adam(lr=self.LR), loss="categorical_crossentropy")

    return model
  
  """def build_model(self, input_dim, output_dim):
    inputs = Input(shape=(input_dim,))

    # ATTENTION PART STARTS HERE
    attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)
    attention_mul = multiply([inputs, attention_probs], name='attention_mul')
    # ATTENTION PART FINISHES HERE

    attention_mul = Dense(64)(attention_mul)
    output = Dense(output_dim, activation='sigmoid')(attention_mul)
    model = Model(input=[inputs], output=output)
    model.compile(optimizer=Adam(lr=self.LR), loss='binary_crossentropy', metrics=['accuracy'])
    return model"""

  def remember(self, estado, acao, reward, proxEstado, done):
    self.memory.append((estado, acao, reward, proxEstado, done))

  def prever(self, estado):
    if(np.random.rand() < self.e):
      return np.random.rand(self.n_servers)
    else:
      estado = np.reshape(estado, [1, self.n_estados])
      return self.model.predict(estado)[0]

  def replay(self, batchSize):
    amostra = random.sample(self.memory, batchSize)
    for estado, acao, reward, proxEstado, done in amostra:
      target = reward
      if not done:
        proxEstado = np.reshape(proxEstado, [1, self.n_estados])
        target = reward + self.gamma * (np.amax(self.model.predict(proxEstado)[0]))

      estado = np.reshape(estado, [1, self.n_estados])
      target_f = self.model.predict(estado)[0]
      target_f = np.reshape(target_f, [1, self.n_servers])
      acao = np.reshape(acao, [1, self.n_servers])
      target_f[0][np.argmax(acao)] = target
      target_f = np.reshape(target_f, [1, self.n_servers])
      actor_loss = self.model.fit(estado, target_f, epochs=1, verbose=0)

    if self.e > self.e_min:
      self.e *= self.e_decay
      
    return actor_loss

import numpy as np
import json
import pandas as pd
import urllib 
import copy
import operator
from collections import defaultdict
from itertools import groupby

class Server():
    def __init__(self,cpu, id_s):
      self.id = id_s
      self.cpu = cpu
      self.ob = 0
      self.cap = cpu
      self.res = 0
      self.ativo = False
      self.lista_tasks = []
      self.rank = 1.0

    
class Task():
    def __init__(self, cpu, start, duracao, id_task):
      self.id = id_task
      self.cpu = cpu
      self.start = start
      self.duracao = float(duracao)
      self.delay = 0.0

class Cloud():
    def __init__(self):
      self.limiar_ob = 1.5
      self.tasks = {}
      self.tasks_original = {}
      response = urllib.request.urlopen("https://raw.githubusercontent.com/renatotnk/IC_Collab/master/tasks_dia_4.json")
      tarefas = json.loads(response.read())
      for key, valor in tarefas.items():
          self.tasks[key] = Task(int(valor["cpu"]), int(valor["start_time"]), int(valor["duration"]), key)
      
      self.tasks = sorted(self.tasks.values(), key = operator.attrgetter("start"))
      self.tasks = self.tasks[:1000]
      self.tasks_original = copy.deepcopy(self.tasks)      
      
    def reset(self):
      self.taskInd = 0
      self.done = False
      self.lista_server = [Server(16, i) for i in range(256)]
      
      self.tasks = copy.deepcopy(self.tasks_original)
      
      observation = []
      
      for server in self.lista_server:

        obs_cpu = float(server.cpu) / server.cap
        observation.append(obs_cpu)

        obs_ob = float(server.ob) / server.cap
        observation.append(obs_ob)

        obs_res = float(server.res) / server.cap
        observation.append(obs_res)
        
        obs_rank = float(server.rank) / 256.0
        observation.append(obs_rank)

      obs_prox_task_cpu = float(self.tasks[self.taskInd].cpu) / 8.0
      observation.append(obs_prox_task_cpu)

      obs_prox_task_dur = float(self.tasks[self.taskInd].duracao) / 14400.0
      observation.append(obs_prox_task_dur)
      
      
      for i in range(len(self.lista_server)):
          observation.append(0.0)
      
      return np.array(observation)      
    
    def step(self,action):
        
      acao = np.argmax(action)
      reward = 0
      
      self.lista_server[acao].lista_tasks.append(self.tasks[self.taskInd])
      self.lista_server[acao].cpu -= self.tasks[self.taskInd].cpu

      
      if self.lista_server[acao].cpu < 0:
          self.lista_server[acao].ob += -(self.lista_server[acao].cpu)
          self.lista_server[acao].cpu = 0
      
      self.lista_server[acao].res = ((self.lista_server[acao].cap - self.lista_server[acao].cpu) + self.lista_server[acao].ob)      
      
      # Caso eu ligue um servidor verifico quantos servidores suportariam a task e puno de acordo
      
####################################### REWARDS #################################################      
      
      #1a reward:
      reward_frag = 0.0
      if not self.lista_server[acao].ativo:
        disponiveis = 0
        for serv in self.lista_server:
          if serv.ativo and serv.cpu >= self.tasks[self.taskInd].cpu:
            disponiveis += 1
        reward_frag = disponiveis/ len(self.lista_server)
      reward_1 = max(0.0, 1.0 - (float(self.lista_server[acao].ob) / float(self.limiar_ob * self.lista_server[acao].cap)) + reward_frag)

      #2a reward:
      reward_2 = max(0.0, 1.0 - (float(self.lista_server[acao].ob) / float(self.limiar_ob * self.lista_server[acao].cap)))

      #3a reward:
      tem_ob = self.lista_server[acao].ob > 0 
      if tem_ob:
        reward_3 = max(0.0, 1.0 - (float(self.lista_server[acao].ob) / float(self.limiar_ob * self.lista_server[acao].cap)))
      else:
        serv_ativo = 0
        serv_sem_ob = 0
        for k in self.lista_server:
          if k.ativo:
            serv_ativo += 1
          if k.cpu > 0:
            serv_sem_ob += 1
        reward_3 = 1 - float(serv_ativo) / serv_sem_ob

      #4a reward:
      if self.lista_server[acao].ativo == False:
          n_ativos = 0
          for serv in self.lista_server:
              if serv.ativo and serv.cpu >= self.tasks[self.taskInd].cpu:
                  n_ativos += 1

          reward_4 = 0.8/(n_ativos + 1)
          self.lista_server[acao].ativo = True
      else:
          if self.lista_server[acao].ob == 0:
              reward_4 = 0.8
          else:
              porcentagem_ob = float(self.lista_server[acao].ob) / self.lista_server[acao].cap
              if porcentagem_ob <= 0.2:
                reward_4 = -5.0 * (porcentagem_ob - 1.0/5.0)**2 + 1
              else:
                reward_4 = 0

      #5a reward:
      def update_ranks(self):
        for sv in self.lista_server:
            if sv.cpu <= 0:
                sv.res = -1.0

        grupos = [list(v) for l,v in groupby(sorted(self.lista_server, key=lambda x:x.res), lambda x: x.res)]

        rank = float(len(grupos))
        for lista in grupos:
          for sv in lista:
            if sv.cpu <= 0:
              sv.rank = -1.0
            else:
              sv.rank = float(rank)
          rank -= 1

        for sv in self.lista_server:
          sv.res = ((sv.cap - sv.cpu) + sv.ob)



      if self.lista_server[acao].rank == -1:
          reward_5 = 0.0
      else:  
          reward_5 = float( 1.0 / float(self.lista_server[acao].rank))

    
####################################### REWARDS #################################################
      self.lista_server[acao].ativo = True
      info = []
      
      self.taskInd += 1
      
      if self.taskInd == len(self.tasks):
         self.done = True
          
          
      if not self.done:
        self.passar_tempo(self.tasks[self.taskInd].start - self.tasks[self.taskInd - 1].start)
      
      observation = []
      if self.taskInd < len(self.tasks):
        for server in self.lista_server:
          obs_cpu = float(server.cpu) / server.cap
          observation.append(obs_cpu)

          obs_ob = float(server.ob) / server.cap
          observation.append(obs_ob)

          obs_res = float(server.res) / server.cap
          observation.append(obs_res)
          
          obs_rank = float(server.rank) / 256.0
          observation.append(obs_rank)
          
        obs_prox_task_cpu = float(self.tasks[self.taskInd].cpu) / 8.0
        observation.append(obs_prox_task_cpu)
        
        obs_prox_task_dur = float(self.tasks[self.taskInd].duracao) / 14400.0
        observation.append(obs_prox_task_dur)
        
        for i in range(len(self.lista_server)):
          if i == acao:
            observation.append(1.0)
          else:
            observation.append(0.0)
          
      if not self.done:    
        self.update_ranks()
      
      return np.array(observation), reward_3, self.done, info
    
    def passar_tempo(self, ticks):
      if ticks < 0:
        print('NOPE')
      
      for i in range(len(self.lista_server)):
          if self.lista_server[i].ativo:  
            for t in self.lista_server[i].lista_tasks:
              steal_time = 1.0 / ((self.lista_server[i].ob / self.lista_server[i].cap) + 1)
              t.duracao -= ticks
              t.delay += steal_time
              
              if t.duracao <= 0:
                self.lista_server[i].ob -= t.cpu
                
                if self.lista_server[i].ob <= 0:
                  self.lista_server[i].cpu += (self.lista_server[i].ob)
                  self.lista_server[i].ob = 0
                  
                self.lista_server[i].lista_tasks.remove(t)
                
                self.lista_server[i].res = ((self.lista_server[i].cap - self.lista_server[i].cpu) + self.lista_server[i].ob)
                
                if len(self.lista_server[i].lista_tasks) == 0:
                  self.lista_server[i].ativo = False
                  
    def update_ranks(self):
      for sv in self.lista_server:
        if sv.cpu <= 0:
          sv.res = -1.0
          
      grupos = [list(v) for l,v in groupby(sorted(self.lista_server, key=lambda x:x.res), lambda x: x.res)]
      
      rank = float(len(grupos))
      for lista in grupos:
        for sv in lista:
          if sv.cpu <= 0:
            sv.rank = -1.0
          else:
            sv.rank = float(rank)
        rank -= 1
        
      for sv in self.lista_server:
        sv.res = ((sv.cap - sv.cpu) + sv.ob)

env = Cloud()
env.reset()
score = 0.0
reward_hist = []
rr = 0
while True:
  acao = np.zeros(256)
  acao[rr % 256] = 1.0
  rr += 1
  obs, reward, done, info = env.step(acao)
  score += reward
  reward_hist.append(reward)
  if done:
    print(score)
    print(np.mean(reward_hist))
    break
    
import matplotlib.pyplot as plt
plt.hist(reward_hist, bins=30)
plt.show()

env = Cloud()
agente = DQN()
loss_over_time = []
score_over_time = []
for i in range(400):
  score = 0.0
  state = env.reset()
  while True:
    acao = agente.prever(state)
    obs, reward, done, info = env.step(acao)
    agente.remember(copy.deepcopy(state), acao, reward, copy.deepcopy(obs), done)
    state = copy.deepcopy(obs)
    score += reward
    if done:
      score_over_time.append(score)
      loss_over_time.append(agente.replay(256))
      #if i == 0 or (i+1) % 50 == 0:
      print("Score: %.2f\tIteração: %d" % (score, i+1))
      break

def reject_outliers(data, m=3):
  return data[abs(data - np.mean(data)) < m * np.std(data)]

print(max(score_over_time))
no_out = reject_outliers(np.array(score_over_time))
plt.plot(range(len(no_out)), no_out)
plt.savefig("Reward_DQN.png", dpi = 450)
plt.show()

plt.plot(range(len([x.history['loss'] for x in loss_over_time])), [x.history['loss'] for x in loss_over_time])
plt.show()
plt.plot(range(len(score_over_time)), score_over_time)
plt.show()

escolhas = np.zeros(256)
agente.e = 0
state = env.reset()
while True:
  acao = agente.prever(state)
  escolhas[np.argmax(acao)] += 1
  observation,reward,done,info = env.step(acao)
  state = copy.deepcopy(observation)
  if done:
    break

plt.bar(range(len(escolhas)), escolhas)
plt.show()

"""#Supervisionado"""

class NN:
  def __init__(self):
    self.LR = 7e-5
    self.n_estados = 770 + 256 + 256
    self.n_servers = 256
    self.model = self.build_model(self.n_estados, self.n_servers)
    self.model.summary()
    
  def build_model(self, input_dim, output_dim):
    inputs = Input(shape=(input_dim,))

    # ATTENTION PART STARTS HERE
    attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)
    attention_mul = multiply([inputs, attention_probs], name='attention_mul')
    # ATTENTION PART FINISHES HERE
    
    for _ in range(6):
      attention_mul = Dense(256)(attention_mul)
    output = Dense(output_dim, activation='sigmoid')(attention_mul)
    model = Model(input=[inputs], output=output)
    model.compile(optimizer=Adam(lr=self.LR), loss='binary_crossentropy', metrics=['accuracy'])
    return model

env = Cloud()
EPISODES = 10

estados = []
acoes = []


for _ in range(EPISODES):
  state = env.reset()
  while True:
    acao = np.zeros(256)
    for s in env.lista_server:
      acao[s.id] = float(1.0/s.rank)

    obs, reward, done, info = env.step(acao)
    estados.append(state)
    acoes.append(acao)
    state = copy.copy(obs)
    if done:
      break

agent = NN()
print (len(estados), len(acoes))
estados = np.reshape(estados, (len(estados), len(estados[0])))
acoes = np.reshape(acoes, (len(acoes), len(acoes[0])))

historico = agent.model.fit(estados, acoes, verbose=0, epochs=150, batch_size=128, shuffle=True)

print(historico.history.keys())
plt.rcParams['figure.figsize'] = [10, 7]
plt.plot(historico.history['loss'])
plt.title('Loss no Treinamento')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.savefig('loss.png', dpi=300)
plt.show()
plt.plot(historico.history['acc'])
plt.title('Acurácia no Treinamento')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.savefig('acc.png', dpi=300)
plt.show()

state = env.reset()
score = 0
escolhas = np.zeros(256)
while True:
  acao = agent.model.predict(np.reshape(state, (1, len(state))))[0]
  escolhas[np.argmax(acao)] += 1
  obs, reward, done, info = env.step(acao)
  score += reward
  state = copy.copy(obs)
  if done:
    print(score)
    break
    
plt.bar(range(len(escolhas)), escolhas)
plt.show()

# ROUND ROBIN
state = env.reset()
score = 0
rr = 0
while True:
  obs, reward, done, info = env.step(rr % 256)
  rr += 1
  score += reward
  if done:
    print(score)
    break

# Algoritmo Ideal
env = Cloud()
state = env.reset()
score = 0
while True:
  escolha = "NaN"
  for sv in env.lista_server:
    if sv.rank == 1.0:
      escolha = sv.id
      break
      
  acao = np.zeros(256)
  acao[escolha] = 1.0
    
  obs, reward, done, info = env.step(acao)
  
  score += reward
  if done:
    print(score)
    break

"""#PPO"""

# Initial framework taken from https://github.com/jaara/AI-blog/blob/master/CartPole-A3C.py

import numpy as np

import gym
import csv

from keras.models import Model
from keras.layers import Input, Dense
from keras import backend as K
from keras.optimizers import Adam

import numba as nb
#from tensorboardX import SummaryWriter

#ENV = 'LunarLander-v2'
CONTINUOUS = True

EPISODES = 800

LOSS_CLIPPING = 0.2 # Only implemented clipping for the surrogate loss, paper said it was best
EPOCHS = 10
NOISE = 0.1 # Exploration noise

GAMMA = 0.9

BUFFER_SIZE = 512
BATCH_SIZE = 256
NUM_ACTIONS = 256
NUM_STATE = 770 + 256 + 256
HIDDEN_SIZE = 128
NUM_LAYERS = 2
ENTROPY_LOSS = 1e-3
LR = 1e-4 # Lower lr stabilises training greatly

DUMMY_ACTION, DUMMY_VALUE = np.zeros((1, NUM_ACTIONS)), np.zeros((1, 1))


@nb.jit
def exponential_average(old, new, b1):
    return old * b1 + (1-b1) * new


def proximal_policy_optimization_loss(advantage, old_prediction):
    def loss(y_true, y_pred):
        prob = y_true * y_pred
        old_prob = y_true * old_prediction
        r = prob/(old_prob + 1e-10)
        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))
    return loss


def proximal_policy_optimization_loss_continuous(advantage, old_prediction):
    def loss(y_true, y_pred):
        var = K.square(NOISE)
        pi = 3.1415926
        denom = K.sqrt(2 * pi * var)
        prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))
        old_prob_num = K.exp(- K.square(y_true - old_prediction) / (2 * var))

        prob = prob_num/denom
        old_prob = old_prob_num/denom
        r = prob/(old_prob + 1e-10)

        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage))
    return loss


class Agent:
    def __init__(self):
        self.critic = self.build_critic()
        if CONTINUOUS is False:
            self.actor = self.build_actor()
        else:
            self.actor = self.build_actor_continuous()

        #self.env = gym.make(ENV)
        #print(self.env.action_space, 'action_space', self.env.observation_space, 'observation_space')
        self.env = Cloud()
        self.episode = 0
        self.observation = self.env.reset()
        self.val = False
        self.reward = []
        self.reward_over_time = []
        self.score_over_time = []
        self.name = self.get_name()
        #self.writer = SummaryWriter(self.name)
        self.gradient_steps = 0

    def get_name(self):
        return 'funciona pls'


    def build_actor(self):
        state_input = Input(shape=(NUM_STATE,))
        advantage = Input(shape=(1,))
        old_prediction = Input(shape=(NUM_ACTIONS,))

        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)
        for _ in range(NUM_LAYERS - 1):
            x = Dense(HIDDEN_SIZE, activation='tanh')(x)

        out_actions = Dense(NUM_ACTIONS, activation='softmax', name='output')(x)

        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])
        model.compile(optimizer=Adam(lr=LR),
                      loss=[proximal_policy_optimization_loss(
                          advantage=advantage,
                          old_prediction=old_prediction)])
        model.summary()

        return model

    def build_actor_continuous(self):
        state_input = Input(shape=(NUM_STATE,))
        advantage = Input(shape=(1,))
        old_prediction = Input(shape=(NUM_ACTIONS,))

        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)
        for _ in range(NUM_LAYERS - 1):
            x = Dense(HIDDEN_SIZE, activation='tanh')(x)

        out_actions = Dense(NUM_ACTIONS, name='output', activation='tanh')(x)

        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])
        model.compile(optimizer=Adam(lr=LR),
                      loss=[proximal_policy_optimization_loss_continuous(
                          advantage=advantage,
                          old_prediction=old_prediction)])
        model.summary()

        return model

    def build_critic(self):

        state_input = Input(shape=(NUM_STATE,))
        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)
        for _ in range(NUM_LAYERS - 1):
            x = Dense(HIDDEN_SIZE, activation='tanh')(x)

        out_value = Dense(1)(x)

        model = Model(inputs=[state_input], outputs=[out_value])
        model.compile(optimizer=Adam(lr=LR), loss='mse')

        return model

    def reset_env(self):
        self.episode += 1
        if self.episode % 100 == 0:
            self.val = True
        else:
            self.val = False
        self.observation = self.env.reset()
        self.reward = []

    def get_action(self):
        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])
        if self.val is False:
            action = np.random.choice(NUM_ACTIONS, p=np.nan_to_num(p[0]))
        else:
            action = np.argmax(p[0])
        action_matrix = np.zeros(NUM_ACTIONS)
        action_matrix[action] = 1
        return action, action_matrix, p

    def get_action_continuous(self):
        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])
        if self.val is False:
            action = action_matrix = p[0] + np.random.normal(loc=0, scale=NOISE, size=p[0].shape)
        else:
            action = action_matrix = p[0]
        return action, action_matrix, p

    def transform_reward(self):
        '''if self.val is True:
            self.writer.add_scalar('Val episode reward', np.array(self.reward).sum(), self.episode)
        else:
            self.writer.add_scalar('Episode reward', np.array(self.reward).sum(), self.episode)'''
        self.score_over_time.append(np.array(self.reward).sum())
        if self.episode == 0 or (self.episode + 1) % 25 == 0:
          print('Episode reward: %.2f %d' % (np.array(self.reward).sum(), self.episode + 1))
        for j in range(len(self.reward) - 2, -1, -1):
            self.reward[j] += self.reward[j + 1] * GAMMA

    def get_batch(self):
        batch = [[], [], [], []]

        tmp_batch = [[], [], []]
        while len(batch[0]) < BUFFER_SIZE:
            if CONTINUOUS is False:
                action, action_matrix, predicted_action = self.get_action()
            else:
                action, action_matrix, predicted_action = self.get_action_continuous()
            observation, reward, done, info = self.env.step(action)
            self.reward.append(reward)

            tmp_batch[0].append(self.observation)
            tmp_batch[1].append(action_matrix)
            tmp_batch[2].append(predicted_action)
            self.observation = observation

            if done:
                self.transform_reward()
                if self.val is False:
                    for i in range(len(tmp_batch[0])):
                        obs, action, pred = tmp_batch[0][i], tmp_batch[1][i], tmp_batch[2][i]
                        r = self.reward[i]
                        batch[0].append(obs)
                        batch[1].append(action)
                        batch[2].append(pred)
                        batch[3].append(r)
                tmp_batch = [[], [], []]
                self.reset_env()

        obs, action, pred, reward = np.array(batch[0]), np.array(batch[1]), np.array(batch[2]), np.reshape(np.array(batch[3]), (len(batch[3]), 1))
        pred = np.reshape(pred, (pred.shape[0], pred.shape[2]))
        return obs, action, pred, reward

    def run(self):
        while self.episode < EPISODES:
            obs, action, pred, reward = self.get_batch()
            obs, action, pred, reward = obs[:BUFFER_SIZE], action[:BUFFER_SIZE], pred[:BUFFER_SIZE], reward[:BUFFER_SIZE]
            old_prediction = pred
            pred_values = self.critic.predict(obs)

            advantage = reward - pred_values
            # advantage = (advantage - advantage.mean()) / advantage.std()
            actor_loss = self.actor.fit([obs, advantage, old_prediction], [action], batch_size=BATCH_SIZE, shuffle=True, epochs=EPOCHS, verbose=False)
            critic_loss = self.critic.fit([obs], [reward], batch_size=BATCH_SIZE, shuffle=True, epochs=EPOCHS, verbose=False)
            #self.writer.add_scalar('Actor loss', actor_loss.history['loss'][-1], self.gradient_steps)
            #self.writer.add_scalar('Critic loss', critic_loss.history['loss'][-1], self.gradient_steps)
            #print("Actor Loss: %.2f\tCritic Loss: %.2f" % (actor_loss.history['loss'][-1], critic_loss.history['loss'][-1]))

            self.gradient_steps += 1


            
if __name__ == '__main__':
    ag = Agent()
    ag.run()
    def reject_outliers(data, m=3):
      return data[abs(data - np.mean(data)) < m * np.std(data)]
    
    
    print(ag.score_over_time)
    with open("rewards.csv", "w") as arq:
      wr = csv.writer(arq, quoting = csv.QUOTE_ALL)
      for sc in ag.score_over_time:
        wr.writerow([sc])
    
    ag.actor.save_weights('deep_ator_pesos.h5')
    ag.critic.save_weights('deep_critico_pesos.h5')
    import matplotlib.pyplot as plt
    print(max(ag.score_over_time))
    no_out = reject_outliers(np.array(ag.score_over_time))
    plt.plot(range(len(no_out)), no_out)
    plt.savefig("Reward.png", dpi = 450)
    plt.show()

def reject_outliers(data, m=3):
  return data[abs(data - np.mean(data)) < m * np.std(data)]

ag.actor.save_weights('deep_ator_pesos.h5')
ag.critic.save_weights('deep_critico_pesos.h5')
import matplotlib.pyplot as plt

no_out = reject_outliers(np.array(ag.score_over_time))
plt.plot(range(len(no_out)), no_out)
plt.savefig("Reward.png", dpi = 450)
plt.show()

ag.actor.load_weights('deep_ator_pesos.h5')
ag.critic.load_weights('deep_critico_pesos.h5')
scores = []
scores_rr = []
ambiente = Cloud()

for _ in range(50):
  state = np.reshape(ambiente.reset(), (1, 1282))
  advantage = DUMMY_VALUE
  action = DUMMY_ACTION
  score = 0
  iteracao = 0
  while True:
    action = ag.actor.predict([state, advantage, action])
    #ambiente.render(iteracao)
    #sleep(1)
    #print(action[0])
    observation, reward, done, info = ambiente.step(action[0])
    advantage = reward - ag.critic.predict(state)[0]
    if not done:
      state = np.reshape(observation, (1, 1282))
    score += reward
    iteracao += 1
    if done:
      print(score)
      scores.append(score)
      break

env = Cloud()
for _ in range(50):
  env.reset()
  score = 0.0
  rr = 0
  while True:
    acao = np.zeros(256)
    acao[rr % 256] = 1.0
    rr += 1
    obs, reward, done, info = env.step(acao)
    score += reward
    if done:
      scores_rr.append(score)
      break
    
import matplotlib.pyplot as plt

plt.plot(range(len(scores)), scores, c = "r")
plt.plot(range(len(scores_rr)), scores_rr, c = "b")
plt.show()

